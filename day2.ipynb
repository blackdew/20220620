{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CNN 복습\n",
        "- 원핫인코딩 없이 sparse_categorical_crossentropy 사용"
      ],
      "metadata": {
        "id": "Q1N8RX1I5DV2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkGpMRwAlx9n"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Syp_sl-7rdya"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(x_train[0].reshape(28, 28), cmap='gray_r')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QS73A3Q4rkCf"
      },
      "outputs": [],
      "source": [
        "X = tf.keras.Input(shape=[28, 28, 1])\n",
        "H = tf.keras.layers.Conv2D(6, 5, padding=\"same\", activation='swish')(X)\n",
        "H = tf.keras.layers.MaxPool2D()(H)\n",
        "H = tf.keras.layers.Conv2D(16, 5, activation='swish')(H)\n",
        "H = tf.keras.layers.MaxPool2D()(H)\n",
        "H = tf.keras.layers.Flatten()(H)\n",
        "H = tf.keras.layers.Dense(120, activation=\"swish\")(H)\n",
        "H = tf.keras.layers.Dense(84, activation=\"swish\")(H)\n",
        "Y = tf.keras.layers.Dense(10, activation=\"softmax\")(H)\n",
        "model = tf.keras.Model(X, Y)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", metrics=\"accuracy\")\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omklmvths-mp"
      },
      "outputs": [],
      "source": [
        "model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Maxpooling 대신 convolution stride=2를 이용하여 사이즈 줄이는 기법"
      ],
      "metadata": {
        "id": "tM60Rx3A5Oef"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7W3bCvsRtNkU"
      },
      "outputs": [],
      "source": [
        "X = tf.keras.Input(shape=[28, 28, 1])\n",
        "H = tf.keras.layers.Conv2D(32, 3, 2, activation='swish')(X)\n",
        "H = tf.keras.layers.Conv2D(64, 3, 2, activation='swish')(H)\n",
        "H = tf.keras.layers.Conv2D(128, 3, activation='swish')(H)\n",
        "H = tf.keras.layers.Flatten()(H)\n",
        "Y = tf.keras.layers.Dense(10, activation=\"softmax\")(H)\n",
        "model1 = tf.keras.Model(X, Y)\n",
        "model1.compile(loss=\"sparse_categorical_crossentropy\", metrics=\"accuracy\")\n",
        "model1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2)"
      ],
      "metadata": {
        "id": "vUYxa1YV2X-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF Dataset 사용법\n"
      ],
      "metadata": {
        "id": "oWZP2pr-5CGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "datasets = datasets.shuffle(1000).batch(128)"
      ],
      "metadata": {
        "id": "yNaFmgmh2bwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_batch, y_batch = next(iter(datasets))\n",
        "print(x_batch.shape, y_batch.shape)\n",
        "print(y_batch[:10])"
      ],
      "metadata": {
        "id": "S2dOsR0j5mlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(datasets, epochs=10)"
      ],
      "metadata": {
        "id": "UabdIEMZ5uIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in range(10):\n",
        "    print(f\"{e} epoch\")\n",
        "    for i, (x_batch, y_batch) in enumerate(datasets):\n",
        "        result = model.train_on_batch(x_batch, y_batch)\n",
        "        if i % 50 == 0:\n",
        "            print(f\"{i}th, {result}\")\n",
        "    model.evaluate(x_test, y_test)\n",
        "    print()"
      ],
      "metadata": {
        "id": "HbWheKJO5_6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# python multi assign & zip "
      ],
      "metadata": {
        "id": "fBdL2fOh9REo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def aa():\n",
        "    return (1, 2), (3, 4)\n",
        "\n",
        "(a, b), (c, d) = aa()\n",
        "print(a, b, c, d)"
      ],
      "metadata": {
        "id": "B_fImIO16eLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alist = [1, 2, 3, 4]\n",
        "\n",
        "for i, a in enumerate(alist):\n",
        "    print(i, a)\n",
        "\n",
        "print(list(enumerate(alist)))"
      ],
      "metadata": {
        "id": "JHvV_d5k7m3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bdict = {\"a\": 1, \"b\": 2, \"c\": 3}\n",
        "\n",
        "for k, v in bdict.items():\n",
        "    print(k, v)\n",
        "\n",
        "print(list(bdict.items()))"
      ],
      "metadata": {
        "id": "X9OHQZZt8Spu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = [1, 2, 3, 4]\n",
        "b = [\"a\", \"b\", \"c\", \"d\"]\n",
        "c = [6, 7, 8, 9]\n",
        "for d, e, f in zip(a, b, c):\n",
        "    print(d, e, f)\n",
        "\n",
        "print(list(zip(a, b, c)))"
      ],
      "metadata": {
        "id": "E0yGlcUB8pNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GradientTape 사용하기"
      ],
      "metadata": {
        "id": "vqFb_cTvAx3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Model(X, Y)\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "for e in range(10):\n",
        "    print(f\"{e} epoch\")\n",
        "    for i, (x_batch, y_batch) in enumerate(datasets):\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = model(x_batch, training=True)\n",
        "            loss = loss_fn(y_batch, y_pred)\n",
        "            grads = tape.gradient(loss, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "        accuracy.update_state(y_batch, y_pred)\n",
        "\n",
        "        if i % 50 == 0:\n",
        "            print(f\"{i} batch, {loss}, {accuracy.result()}\")"
      ],
      "metadata": {
        "id": "hsvq6dFS869W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Model(X, Y)\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "@tf.function\n",
        "def train_step(x_batch, y_batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = model(x_batch, training=True)\n",
        "        loss = loss_fn(y_batch, y_pred)\n",
        "        grads = tape.gradient(loss, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    accuracy.update_state(y_batch, y_pred)\n",
        "    return loss, accuracy.result()\n",
        "\n",
        "for e in range(10):\n",
        "    print(f\"{e} epoch\")\n",
        "    for i, (x_batch, y_batch) in enumerate(datasets):\n",
        "        loss, acc = train_step(x_batch, y_batch)\n",
        "        if i % 50 == 0:\n",
        "            print(f\"{i} batch, {loss}, {accuracy.result()}\")"
      ],
      "metadata": {
        "id": "hEl2UNRPBm7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom model class"
      ],
      "metadata": {
        "id": "isexSKq3FRcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_cnn():\n",
        "    X = tf.keras.Input(shape=[28, 28, 1])\n",
        "    H = tf.keras.layers.Conv2D(32, 5, 2, activation='swish')(X)\n",
        "    H = tf.keras.layers.Conv2D(64, 5, 2, activation='swish')(H)\n",
        "    H = tf.keras.layers.Conv2D(128, 5, activation='swish')(H)\n",
        "    H = tf.keras.layers.Flatten()(H)\n",
        "    Y = tf.keras.layers.Dense(10, activation=\"softmax\")(H)\n",
        "    return tf.keras.Model(X, Y)"
      ],
      "metadata": {
        "id": "eiMoaLWODsdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Custom(tf.keras.Model):\n",
        "    def __init__(self, cnn):\n",
        "        super(Custom, self).__init__()\n",
        "        self.compile()\n",
        "        self.cnn = cnn\n",
        "        self.optimizer = tf.keras.optimizers.Adam()\n",
        "        self.custom_loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "        self.loss_metric = tf.keras.metrics.Mean()\n",
        "        self.acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "    \n",
        "    def update_metrics(self, loss, y_batch, y_pred):\n",
        "        self.loss_metric.update_state(loss)\n",
        "        self.acc_metric.update_state(y_batch, y_pred)\n",
        "\n",
        "    def train_step(self, batch):\n",
        "        x_batch, y_batch = batch\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self.cnn(x_batch, training=True)\n",
        "            loss = self.custom_loss(y_batch, y_pred)\n",
        "            grads = tape.gradient(loss, self.cnn.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.cnn.trainable_weights))\n",
        "\n",
        "        self.update_metrics(loss, y_batch, y_pred)\n",
        "        return {\"loss\": self.loss_metric.result(), \"accuracy\": self.acc_metric.result()}"
      ],
      "metadata": {
        "id": "BEe4fA5LFcW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = make_cnn()\n",
        "model = Custom(cnn)\n",
        "model.fit(datasets, epochs=10)"
      ],
      "metadata": {
        "id": "yo3vanP6F2ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CIFAR-10"
      ],
      "metadata": {
        "id": "LrAD88L2c0Nj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "print(x_train.shape, y_train.shape)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(x_train[1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7_XUFV97F9xB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_cnn():\n",
        "    X = tf.keras.Input(shape=[32, 32, 3])\n",
        "\n",
        "    H = tf.keras.layers.Conv2D(32, 5, 2)(X)\n",
        "    H = tf.keras.layers.BatchNormalization()(H)\n",
        "    H = tf.keras.layers.Activation(\"swish\")(H)\n",
        "\n",
        "    H = tf.keras.layers.Conv2D(64, 5, 2)(H)\n",
        "    H = tf.keras.layers.BatchNormalization()(H)\n",
        "    H = tf.keras.layers.Activation(\"swish\")(H)\n",
        "\n",
        "    H = tf.keras.layers.Conv2D(128, 5)(H)\n",
        "    H = tf.keras.layers.BatchNormalization()(H)\n",
        "    H = tf.keras.layers.Activation(\"swish\")(H)\n",
        "\n",
        "    H = tf.keras.layers.Flatten()(H)\n",
        "    Y = tf.keras.layers.Dense(10, activation=\"softmax\")(H)\n",
        "    return tf.keras.Model(X, Y)"
      ],
      "metadata": {
        "id": "9ZwHSDWHcsf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Custom(tf.keras.Model):\n",
        "    def __init__(self, cnn):\n",
        "        super(Custom, self).__init__()\n",
        "        self.compile()\n",
        "        self.cnn = cnn\n",
        "        self.optimizer = tf.keras.optimizers.Adam()\n",
        "        self.custom_loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "        self.loss_metric = tf.keras.metrics.Mean()\n",
        "        self.acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "    \n",
        "    def update_metrics(self, loss, y_batch, y_pred):\n",
        "        self.loss_metric.update_state(loss)\n",
        "        self.acc_metric.update_state(y_batch, y_pred)\n",
        "\n",
        "    def train_step(self, batch):\n",
        "        x_batch, y_batch = batch\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self.cnn(x_batch, training=True)\n",
        "            loss = self.custom_loss(y_batch, y_pred)\n",
        "            grads = tape.gradient(loss, self.cnn.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.cnn.trainable_weights))\n",
        "\n",
        "        self.update_metrics(loss, y_batch, y_pred)\n",
        "        return {\"loss\": self.loss_metric.result(), \"accuracy\": self.acc_metric.result()}\n",
        "        \n",
        "    def test_step(self, batch):\n",
        "        x_batch, y_batch = batch\n",
        "        y_pred = self.cnn(x_batch)\n",
        "        loss = self.custom_loss(y_batch, y_pred)\n",
        "        self.update_metrics(loss, y_batch, y_pred)\n",
        "        return {\"loss\": self.loss_metric.result(), \"accuracy\": self.acc_metric.result()}"
      ],
      "metadata": {
        "id": "C46nuVK-h7lM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = make_cnn()\n",
        "model = Custom(cnn)\n",
        "model.fit(x_train, y_train, batch_size=128, epochs=10, validation_split=0.2)"
      ],
      "metadata": {
        "id": "XymfVXqTh8PU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "datasets = datasets.shuffle(1000).batch(128)\n",
        "\n",
        "valid = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "valid = valid.shuffle(1000).batch(128)"
      ],
      "metadata": {
        "id": "fKW4V625iAFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = make_cnn()\n",
        "model = Custom(cnn)\n",
        "model.fit(datasets, epochs=10, validation_data=valid)"
      ],
      "metadata": {
        "id": "0QbFnzDulMGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.constant(3.0)\n",
        "with tf.GradientTape() as g:\n",
        "    g.watch(x)\n",
        "    y = x * x\n",
        "    dy = g.gradient(y, x)\n",
        "print(dy)"
      ],
      "metadata": {
        "id": "6oIx6XgglSst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# vanillaGAN"
      ],
      "metadata": {
        "id": "8D140LhL6m4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = (x_train - 127.5) / 127.5\n",
        "print(x_train.shape, x_train.min(), x_train.max())\n"
      ],
      "metadata": {
        "id": "L7bsKubypCTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_discriminator():\n",
        "    dx = tf.keras.Input(shape=[28 * 28])\n",
        "    dh = tf.keras.layers.Dense(128)(dx)\n",
        "    dh = tf.keras.layers.LeakyReLU(alpha=0.2)(dh)\n",
        "    dh = tf.keras.layers.Dense(128)(dh)\n",
        "    dh = tf.keras.layers.LeakyReLU(alpha=0.2)(dh)\n",
        "    dy = tf.keras.layers.Dense(1, activation='sigmoid')(dh)\n",
        "    return tf.keras.Model(dx, dy)\n",
        "\n",
        "def make_generator():\n",
        "    gx = tf.keras.Input(shape=[100])\n",
        "    gh = tf.keras.layers.Dense(128)(gx)\n",
        "    gh = tf.keras.layers.BatchNormalization()(gh)\n",
        "    gh = tf.keras.layers.Activation('swish')(gh)\n",
        "    gh = tf.keras.layers.Dense(128)(gx)\n",
        "    gh = tf.keras.layers.BatchNormalization()(gh)\n",
        "    gh = tf.keras.layers.Activation('swish')(gh)\n",
        "    gy = tf.keras.layers.Dense(28 * 28, activation='tanh')(gh)\n",
        "    return tf.keras.Model(gx, gy)\n"
      ],
      "metadata": {
        "id": "NP5ICvZ-68S_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VanillaGAN(tf.keras.Model):\n",
        "    def __init__(self, g, d):\n",
        "        super(VanillaGAN, self).__init__()\n",
        "        self.compile()\n",
        "\n",
        "        self.generator = g\n",
        "        self.discriminator = d\n",
        "\n",
        "        self.d_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "        self.g_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "\n",
        "        self.d_loss_metrics = tf.keras.metrics.Mean()\n",
        "        self.d_acc_metrics = tf.keras.metrics.BinaryAccuracy()\n",
        "        self.g_loss_metrics = tf.keras.metrics.Mean()\n",
        "        self.g_acc_metrics = tf.keras.metrics.BinaryAccuracy()\n",
        "\n",
        "    def disc_loss(self, y_real, y_fake):\n",
        "        real_loss = tf.keras.losses.binary_crossentropy(tf.ones_like(y_real), y_real)\n",
        "        fake_loss = tf.keras.losses.binary_crossentropy(tf.zeros_like(y_fake), y_fake)\n",
        "        return real_loss + fake_loss\n",
        "    \n",
        "    def gen_loss(self, y_fake):\n",
        "        g_loss = tf.keras.losses.binary_crossentropy(tf.ones_like(y_fake), y_fake)\n",
        "        return g_loss\n",
        "\n",
        "    def update_metrics(self, y_real, y_fake, g_loss, d_loss):\n",
        "        self.d_loss_metrics.update_state(d_loss)\n",
        "        self.d_acc_metrics.update_state(tf.ones_like(y_real), y_real)\n",
        "        self.d_acc_metrics.update_state(tf.zeros_like(y_fake), y_fake)\n",
        "        self.g_loss_metrics.update_state(g_loss)\n",
        "        self.g_acc_metrics.update_state(tf.ones_like(y_fake), y_fake)\n",
        "\n",
        "    def train_step(self, real):\n",
        "        noise = tf.random.normal([tf.shape(real)[0], 100])\n",
        "        with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\n",
        "            fake = self.generator(noise, training=True)\n",
        "            y_real = self.discriminator(real, training=True)\n",
        "            y_fake = self.discriminator(fake, training=True)\n",
        "\n",
        "            g_loss = self.gen_loss(y_fake)\n",
        "            d_loss = self.disc_loss(y_real, y_fake)\n",
        "\n",
        "            g_grad = g_tape.gradient(g_loss, self.generator.trainable_variables)    \n",
        "            d_grad = d_tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
        "\n",
        "        self.g_optimizer.apply_gradients(zip(g_grad, self.generator.trainable_variables))\n",
        "        self.d_optimizer.apply_gradients(zip(d_grad, self.discriminator.trainable_variables))\n",
        "\n",
        "        self.update_metrics(y_real, y_fake, g_loss, d_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metrics.result(),\n",
        "            \"d_acc\": self.d_acc_metrics.result(),\n",
        "            \"g_loss\": self.g_loss_metrics.result(),\n",
        "            \"g_acc\": self.g_acc_metrics.result(),\n",
        "        }"
      ],
      "metadata": {
        "id": "xCBkUQ6-7Ej2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "class Monitor(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        x_rnd = np.random.randn(5 * 100).reshape(5, 100)\n",
        "        generated = self.model.generator(x_rnd)\n",
        "        for i in range(5):\n",
        "            plt.subplot(1, 5, 1 + i)\n",
        "            plt.axis('off')\n",
        "            plt.imshow(generated[i].numpy().reshape(28, 28), cmap='gray_r')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "h4p8BgzT9nMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = make_discriminator()\n",
        "generator = make_generator()\n",
        "gen = VanillaGAN(generator, discriminator)\n",
        "gen.fit(x_train.reshape(-1, 28 * 28), epochs=50, batch_size=128, callbacks=[Monitor()])"
      ],
      "metadata": {
        "id": "MVek4m7G-IP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 작은 차원의 분포 학습"
      ],
      "metadata": {
        "id": "u-_AZR1FIyCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VanillaGAN(tf.keras.Model):\n",
        "    def __init__(self, g, d):\n",
        "        super(VanillaGAN, self).__init__()\n",
        "        self.compile()\n",
        "\n",
        "        self.generator = g\n",
        "        self.discriminator = d\n",
        "\n",
        "        self.d_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "        self.g_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "\n",
        "        self.d_loss_metrics = tf.keras.metrics.Mean()\n",
        "        self.d_acc_metrics = tf.keras.metrics.BinaryAccuracy()\n",
        "        self.g_loss_metrics = tf.keras.metrics.Mean()\n",
        "        self.g_acc_metrics = tf.keras.metrics.BinaryAccuracy()\n",
        "\n",
        "    def disc_loss(self, y_real, y_fake):\n",
        "        real_loss = tf.keras.losses.binary_crossentropy(tf.ones_like(y_real), y_real)\n",
        "        fake_loss = tf.keras.losses.binary_crossentropy(tf.zeros_like(y_fake), y_fake)\n",
        "        return real_loss + fake_loss\n",
        "    \n",
        "    def gen_loss(self, y_fake):\n",
        "        g_loss = tf.keras.losses.binary_crossentropy(tf.ones_like(y_fake), y_fake)\n",
        "        return g_loss\n",
        "\n",
        "    def update_metrics(self, y_real, y_fake, g_loss, d_loss):\n",
        "        self.d_loss_metrics.update_state(d_loss)\n",
        "        self.d_acc_metrics.update_state(tf.ones_like(y_real), y_real)\n",
        "        self.d_acc_metrics.update_state(tf.zeros_like(y_fake), y_fake)\n",
        "        self.g_loss_metrics.update_state(g_loss)\n",
        "        self.g_acc_metrics.update_state(tf.ones_like(y_fake), y_fake)\n",
        "\n",
        "    def train_step(self, real):\n",
        "        noise = tf.random.normal([tf.shape(real)[0], 5])\n",
        "        with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\n",
        "            fake = self.generator(noise, training=True)\n",
        "            y_real = self.discriminator(real, training=True)\n",
        "            y_fake = self.discriminator(fake, training=True)\n",
        "\n",
        "            g_loss = self.gen_loss(y_fake)\n",
        "            d_loss = self.disc_loss(y_real, y_fake)\n",
        "\n",
        "            g_grad = g_tape.gradient(g_loss, self.generator.trainable_variables)    \n",
        "            d_grad = d_tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
        "\n",
        "        self.g_optimizer.apply_gradients(zip(g_grad, self.generator.trainable_variables))\n",
        "        self.d_optimizer.apply_gradients(zip(d_grad, self.discriminator.trainable_variables))\n",
        "\n",
        "        self.update_metrics(y_real, y_fake, g_loss, d_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metrics.result(),\n",
        "            \"d_acc\": self.d_acc_metrics.result(),\n",
        "            \"g_loss\": self.g_loss_metrics.result(),\n",
        "            \"g_acc\": self.g_acc_metrics.result(),\n",
        "        }"
      ],
      "metadata": {
        "id": "diOQR5FXJvND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_discriminator():\n",
        "    dx = tf.keras.Input(shape=[2])\n",
        "    dh = tf.keras.layers.Dense(25, activation='relu')(dx)\n",
        "    dy = tf.keras.layers.Dense(1, activation='sigmoid')(dh)\n",
        "    return tf.keras.Model(dx, dy)\n",
        "\n",
        "def make_generator():\n",
        "    gx = tf.keras.Input(shape=[5])\n",
        "    gh = tf.keras.layers.Dense(15, activation=\"swish\")(gx)\n",
        "    gy = tf.keras.layers.Dense(2)(gh)\n",
        "    return tf.keras.Model(gx, gy)\n"
      ],
      "metadata": {
        "id": "6CiChzI9JE-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "class Monitor(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        x_rnd = np.random.randn(1000).reshape(200, 5)\n",
        "        generated = self.model.generator(x_rnd)\n",
        "        plt.scatter(x_train[:1000, 0], x_train[:1000, 1], color='red')\n",
        "        plt.scatter(generated[:, 0], generated[:, 1], color=\"blue\")\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "TeDUhmjRJ4j0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.random.randn(120000).reshape(60000, 2)\n",
        "print(x_train.shape)\n",
        "\n",
        "x_train[:, 1] = 1 * x_train[:, 0] + 2 * x_train[:, 1]\n",
        "\n",
        "plt.scatter(x_train[:1000, 0], x_train[:1000, 1], color='red')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6uepsWwS-XLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "generator = make_generator()\n",
        "discriminator = make_discriminator()\n",
        "gen = VanillaGAN(generator, discriminator)\n",
        "gen.fit(x_train, epochs=20, batch_size=128, callbacks=[Monitor()])"
      ],
      "metadata": {
        "id": "Kgsx3tuMKYhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pif7gu5QKepc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "day2.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}